{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text using Markov Chains\n",
    "\n",
    "### Aim:\n",
    "In this notebook, we aim to generate similar storyline like Harry Potter by building a 1st Order Markov Chains around all the books of Harry Potter at word level. To know in detail the working of the model read the related blog here.\n",
    "\n",
    "### Author:\n",
    "1. [Prakhar Mishra](https://www.linkedin.com/in/prakhar21/)\n",
    "\n",
    "__For more such materials follow me on __[Medium](https://medium.com/@prakhar.mishra)\n",
    "\n",
    "### Resources:\n",
    "1. [Markov Chain explained Visually](http://setosa.io/ev/markov-chains/)\n",
    "2. [Markov Chains in Python](https://www.datacamp.com/community/tutorials/markov-chains-python-tutorial)\n",
    "3. [Markov Chains (YouTube)](https://www.youtube.com/watch?v=uvYTGEZQTEs)\n",
    "\n",
    "### Improvement Scope\n",
    "1. Try higher order markov chains (maybe 2 or 3)\n",
    "2. Try increasing vocabulary of words in the current one.\n",
    "3. Try tuning the exploration factor i.e. randomness_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import operator\n",
    "import codecs\n",
    "import random\n",
    "import nltk\n",
    "import glob\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that encapsulates all the functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextGenerator(object):\n",
    "    \n",
    "    def __init__(self, data_list):\n",
    "        self.text = self._load(data_list)\n",
    "        self.text_tokens = self._prune(self._tokenize())\n",
    "        self.states = list(set(self.text_tokens))\n",
    "        self.possible_transitions = self._get_transitions()\n",
    "        self.trasnsition_probabilites = self.train()\n",
    "        self.total_words = 0.0\n",
    "        \n",
    "    def _load(self, files):\n",
    "        text = \" \"\n",
    "        for f in files:\n",
    "            print 'Reading {}'.format(f)\n",
    "            with codecs.open(f, 'rb', 'utf-8') as infile:\n",
    "                text += self._clean(infile.read().encode('utf-8').decode('ascii', 'ignore')).strip()\n",
    "        return text\n",
    "    \n",
    "    def _get_possibilities(self, state):\n",
    "        words = []\n",
    "        for index, value in enumerate(self.text_tokens):\n",
    "            if value == state:\n",
    "                try:\n",
    "                    words.append(self.text_tokens[index+1])\n",
    "                except:\n",
    "                    words.append('EOS')\n",
    "        return {state: dict(collections.Counter(words))}\n",
    "    \n",
    "    def _add_probabilities(self, possibilities):\n",
    "        temp = {}\n",
    "        for possibility in possibilities:\n",
    "            for k, v in possibility.items():\n",
    "                temp[k] = [{'probab': (count/float(self.total_words)) * (1/float(len(v))), 'word': wrd} for wrd,count in v.items()]\n",
    "        return temp\n",
    "    \n",
    "    def train(self):\n",
    "        possibilities = []\n",
    "        for state in self.states:\n",
    "            possibilities.append(self._get_possibilities(state))\n",
    "        probabilities = self._add_probabilities(possibilities)\n",
    "        return probabilities\n",
    "            \n",
    "    def _get_transitions(self):\n",
    "        return [[self.states] for state in self.states]\n",
    "    \n",
    "    def _prune(self, tokens):\n",
    "        if len(tokens) > 100000:\n",
    "            self.total_words = 100000\n",
    "            return tokens[:self.total_words]\n",
    "        self.total_words = len(tokens)\n",
    "        return tokens\n",
    "        \n",
    "    def _clean(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"(\\n|\\t|/)\", \" \", text)\n",
    "        text = re.sub(r'([.,/#!$%^&*;:{}=_`~()-])[.,/#!$%^&*;:{}=_`~()-]+', r'\\1', text)\n",
    "        text = re.sub('([.,!?()])', r' \\1 ', text)\n",
    "        return re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    def get_len(self, d):\n",
    "        return len(d)\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        tokens = nltk.word_tokenize(self.text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Calculating Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/prakhar/blogs/markov_tg/data/Book 1 - The Philosopher's Stone.txt\n",
      "Reading /home/prakhar/blogs/markov_tg/data/Book 7 - The Deathly Hallows.txt\n",
      "Reading /home/prakhar/blogs/markov_tg/data/Book 6 - The Half Blood Prince.txt\n",
      "Reading /home/prakhar/blogs/markov_tg/data/Book 4 - The Goblet of Fire.txt\n",
      "Reading /home/prakhar/blogs/markov_tg/data/Book 2 - The Chamber of Secrets.txt\n",
      "Reading /home/prakhar/blogs/markov_tg/data/Book 3 - The Prisoner of Azkaban.txt\n",
      "Reading /home/prakhar/blogs/markov_tg/data/Book 5 - The Order of the Phoenix.txt\n",
      "Length of the training file 6592904\n",
      "Number of words 100000\n",
      "[u'wrought-iron', u'yellow', u'four', u'woods', u'spiders', u'hanging', u'humming', u'wizardry', u'lord', u'flicking', u'three-thirty', u'sinking', u'figg', u'foul', u'screaming']\n"
     ]
    }
   ],
   "source": [
    "# preparing the data for training the model\n",
    "\n",
    "data = os.path.abspath('data')\n",
    "files = glob.glob(data+'/*')\n",
    "\n",
    "generator = TextGenerator(files)\n",
    "\n",
    "# length of the text\n",
    "print \"Length of the training file {}\".format(generator.get_len(generator.text))\n",
    "print \"Number of words {}\".format(generator.get_len(generator.text_tokens))\n",
    "print generator.states[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Story using the patterns observed from Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoenix feather duster at all right. he had a bit more to be in his face. rowling he was going to the other, said ron and a few feet, but he said ron and the philosophers stone - j. he said harry potter and the philosophers stone - j. he had a large ice cream on the door and a bit, but i know, and hermione. he had been to the door, and hermione.\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "def formatter(s):\n",
    "    s = s.split()\n",
    "    # greedy sentence finisher (matches to last (.))\n",
    "    s = ' '.join(s[:[idx for idx, ch in enumerate(s) if ch == '.'][-1]+1])\n",
    "    s = s.capitalize()  # sentence casing\n",
    "    s = re.sub(r'\\s(\\.|,|!|\\?|\\(|\\)|\\]|\\[)', r'\\1', s) # remove padded space before punc.\n",
    "    return s\n",
    "\n",
    "seed_word = 'phoenix'\n",
    "story = [seed_word]\n",
    "words = 0\n",
    "max_words = 100\n",
    "randomness_level = 3\n",
    "\n",
    "while words < max_words-1:\n",
    "    words += 1    \n",
    "    candidates = generator.trasnsition_probabilites.get(seed_word)\n",
    "    temp = sorted(candidates, reverse=True)\n",
    "    candidates = [i.get('probab') for i in temp]\n",
    "    grouped = sum([i[1] for i in [(k, sum(1 for i in g)) for k,g in itertools.groupby(candidates)][:randomness_level]])\n",
    "    seed_word = random.choice(temp[:grouped]).get('word')\n",
    "    story.append(seed_word)\n",
    "    \n",
    "print formatter(' '.join(story).encode('utf-8').decode('ascii', 'ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
